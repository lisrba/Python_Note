{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim%20Quick%20Start.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_corpus = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in raw_corpus]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        print(frequency[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisrba\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['time', 'survey', 'graph', 'system', 'human']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 4, 'survey': 6, 'graph': 10, 'system': 7, 'human': 1, 'minors': 11, 'trees': 9, 'interface': 0, 'response': 5, 'eps': 8, 'user': 3, 'computer': 2}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"interaction\" did not occur in the original corpus and so it was not included in the vectorization. Also note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(0, 1), (3, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (7, 2), (8, 1)],\n",
       " [(3, 1), (4, 1), (5, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(6, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term referring to a transformation from one document representation to another. In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned from the training corpus.\n",
    "One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.5898341626740045), (11, 0.8075244024440723)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "# transform the \"system minors\" sting\n",
    "tfidf[dictionary.doc2bow(\"system minors\".lower().split())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisrba\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-01-07 10:54:41,235 : INFO : collecting all words and their counts\n",
      "2017-01-07 10:54:41,235 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-07 10:54:41,236 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2017-01-07 10:54:41,236 : INFO : Loading a fresh vocabulary\n",
      "2017-01-07 10:54:41,237 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-01-07 10:54:41,238 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-01-07 10:54:41,239 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-01-07 10:54:41,239 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-01-07 10:54:41,240 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-01-07 10:54:41,240 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-01-07 10:54:41,242 : INFO : resetting layer weights\n",
      "2017-01-07 10:54:41,242 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-07 10:54:41,243 : INFO : expecting 2 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-07 10:54:41,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-07 10:54:41,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-07 10:54:41,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-07 10:54:41,249 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-01-07 10:54:41,249 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some toy data to use with the following example\n",
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f1.txt', './data/f2.txt']\n",
    "\n",
    "for i,fname in enumerate(filenames):\n",
    "    with smart_open.smart_open(fname, 'w') as fout:\n",
    "        for line in sentences[i]:\n",
    "            fout.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python流式高效访问(超)大文件的库(支持云端/本地的压缩/未压缩文件：S3, HDFS, gzip, bz2...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enumerate(iterable, start=0)¶：Return an enumerate object. iterable must be a sequence, an iterator, or some other object which supports iteration. The __next__() method of the iterator returned by enumerate() returns a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable.\n",
    "\n",
    "seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "\n",
    "list(enumerate(seasons))\n",
    "\n",
    "[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n",
    "\n",
    "list(enumerate(seasons, start=1))\n",
    "\n",
    "[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with as 的說明：\n",
    "http://blog.kissdata.com/2014/05/23/python-with.html\n",
    "取代掉try catch最後finally要close的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如下面的程式，在 Windows 上會產生 a\\b\\c，在 Linux 上會產生 a/b/c：\n",
    "\n",
    "import os\n",
    "print os.path.join(\"a\", \"b\", \"c\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yield說明：http://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MySentences object at 0x000002648F5C97F0>\n",
      "[['first'], ['sentence'], ['second'], ['sentence']]\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentences('./data/') # a memory-friendly iterator\n",
    "print(sentences)\n",
    "print(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 10:56:10,956 : INFO : collecting all words and their counts\n",
      "2017-01-07 10:56:10,957 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-07 10:56:10,958 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2017-01-07 10:56:10,959 : INFO : Loading a fresh vocabulary\n",
      "2017-01-07 10:56:10,959 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-01-07 10:56:10,960 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-01-07 10:56:10,960 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-01-07 10:56:10,961 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-01-07 10:56:10,962 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-01-07 10:56:10,962 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-01-07 10:56:10,963 : INFO : resetting layer weights\n",
      "2017-01-07 10:56:10,964 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-07 10:56:10,964 : INFO : expecting 4 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-07 10:56:10,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-07 10:56:10,970 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-07 10:56:10,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-07 10:56:10,972 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-01-07 10:56:10,972 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# generate the Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'first': <gensim.models.word2vec.Vocab object at 0x000002648F5C97B8>, 'second': <gensim.models.word2vec.Vocab object at 0x000002648F5C9898>, 'sentence': <gensim.models.word2vec.Vocab object at 0x000002648F5C96A0>}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)\n",
    "\n",
    "sg defines the training algorithm. By default (sg=0), CBOW is used. Otherwise (sg=1), skip-gram is employed.\n",
    "\n",
    "size is the dimensionality of the feature vectors.\n",
    "\n",
    "window is the maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "alpha is the initial learning rate (will linearly drop to min_alpha as training progresses).\n",
    "\n",
    "seed = for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization.)\n",
    "\n",
    "min_count = ignore all words with total frequency lower than this.\n",
    "\n",
    "max_vocab_size = limit RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit (default).\n",
    "\n",
    "sample = threshold for configuring which higher-frequency words are randomly downsampled;\n",
    "default is 1e-3, useful range is (0, 1e-5).\n",
    "workers = use this many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "hs = if 1, hierarchical softmax will be used for model training. If set to 0 (default), and negative is non-zero, negative sampling will be used.\n",
    "\n",
    "negative = if > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). Default is 5. If set to 0, no negative samping is used.\n",
    "\n",
    "cbow_mean = if 0, use the sum of the context word vectors. If 1 (default), use the mean. Only applies when cbow is used.\n",
    "\n",
    "hashfxn = hash function to use to randomly initialize weights, for increased training reproducibility. Default is Python’s rudimentary built in hash function.\n",
    "\n",
    "iter = number of iterations (epochs) over the corpus. Default is 5.\n",
    "\n",
    "trim_rule = vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count). Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and returns either utils.RULE_DISCARD, utils.RULE_KEEP or utils.RULE_DEFAULT. Note: The rule, if given, is only used prune vocabulary during build_vocab() and is not stored as part of the model.\n",
    "\n",
    "sorted_vocab = if 1 (default), sort the vocabulary by descending frequency before assigning word indexes.\n",
    "\n",
    "batch_words = target size (in words) for batches of examples passed to worker threads (and thus cython routines). Default is 10000. (Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 10:56:27,908 : INFO : collecting all words and their counts\n",
      "2017-01-07 10:56:27,909 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-07 10:56:27,910 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2017-01-07 10:56:27,911 : INFO : Loading a fresh vocabulary\n",
      "2017-01-07 10:56:27,911 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-01-07 10:56:27,912 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-01-07 10:56:27,912 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-01-07 10:56:27,913 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-01-07 10:56:27,914 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-01-07 10:56:27,914 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-01-07 10:56:27,915 : INFO : resetting layer weights\n",
      "2017-01-07 10:56:27,916 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-07 10:56:27,916 : INFO : expecting 4 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-07 10:56:27,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-07 10:56:27,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-07 10:56:27,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-07 10:56:27,924 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-01-07 10:56:27,924 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training\n",
    "new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     \n",
    "new_model.train(sentences)                       # can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)¶\n",
    "\n",
    "Build vocabulary from a sequence of sentences (can be a once-only generator stream). \n",
    "\n",
    "Each sentence must be a list of unicode strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'first': <gensim.models.word2vec.Vocab object at 0x000002648F5C97B8>, 'second': <gensim.models.word2vec.Vocab object at 0x000002648F5C9898>, 'sentence': <gensim.models.word2vec.Vocab object at 0x000002648F5C96A0>}\n"
     ]
    }
   ],
   "source": [
    "print(new_model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data would be nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\n",
      "C:\\Users\\lisrba\\Anaconda3\\lib\\site-packages\\gensim\n"
     ]
    }
   ],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "print(os.sep)\n",
    "print(gensim.__path__[0])\n",
    "\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. os.sep is the (or a most common) pathname separator ('/' or ':' or '\\') \n",
    "\n",
    "2. import os \n",
    "print os.sep \n",
    "Your output will be '\\'or '/' depending on the operating system you're using.\n",
    "\n",
    "3. makes your code more portable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyText object at 0x000002648F5C9630>\n"
     ]
    }
   ],
   "source": [
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower().split()\n",
    "\n",
    "sentences = MyText()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 10:59:57,388 : INFO : collecting all words and their counts\n",
      "2017-01-07 10:59:57,390 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-07 10:59:57,414 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-01-07 10:59:57,415 : INFO : Loading a fresh vocabulary\n",
      "2017-01-07 10:59:57,422 : INFO : min_count=10 retains 806 unique words (7% of original 10186, drops 9380)\n",
      "2017-01-07 10:59:57,427 : INFO : min_count=10 leaves 40964 word corpus (68% of original 59890, drops 18926)\n",
      "2017-01-07 10:59:57,434 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-01-07 10:59:57,435 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2017-01-07 10:59:57,436 : INFO : downsampling leaves estimated 26224 word corpus (64.0% of prior 40964)\n",
      "2017-01-07 10:59:57,436 : INFO : estimated required memory for 806 words and 100 dimensions: 1047800 bytes\n",
      "2017-01-07 10:59:57,439 : INFO : resetting layer weights\n",
      "2017-01-07 10:59:57,453 : INFO : training model with 3 workers on 806 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-07 10:59:57,454 : INFO : expecting 300 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-07 10:59:57,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-07 10:59:57,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-07 10:59:57,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-07 10:59:57,602 : INFO : training on 299450 raw words (130859 effective words) took 0.1s, 942020 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of min_count=5\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 10:59:59,686 : INFO : collecting all words and their counts\n",
      "2017-01-07 10:59:59,687 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-07 10:59:59,709 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-01-07 10:59:59,710 : INFO : Loading a fresh vocabulary\n",
      "2017-01-07 10:59:59,719 : INFO : min_count=5 retains 1723 unique words (16% of original 10186, drops 8463)\n",
      "2017-01-07 10:59:59,721 : INFO : min_count=5 leaves 46858 word corpus (78% of original 59890, drops 13032)\n",
      "2017-01-07 10:59:59,729 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-01-07 10:59:59,730 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-01-07 10:59:59,731 : INFO : downsampling leaves estimated 32849 word corpus (70.1% of prior 46858)\n",
      "2017-01-07 10:59:59,732 : INFO : estimated required memory for 1723 words and 200 dimensions: 3618300 bytes\n",
      "2017-01-07 10:59:59,739 : INFO : resetting layer weights\n",
      "2017-01-07 10:59:59,781 : INFO : training model with 3 workers on 1723 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-07 10:59:59,782 : INFO : expecting 300 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-07 10:59:59,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-07 10:59:59,968 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-07 10:59:59,973 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-07 10:59:59,973 : INFO : training on 299450 raw words (164161 effective words) took 0.2s, 869477 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of size=100\n",
    "model = gensim.models.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-06 20:42:49,043 : INFO : family: 0.0% (0/2)\n",
      "2017-01-06 20:42:49,064 : INFO : gram3-comparative: 0.0% (0/12)\n",
      "2017-01-06 20:42:49,086 : INFO : gram4-superlative: 0.0% (0/12)\n",
      "2017-01-06 20:42:49,103 : INFO : gram5-present-participle: 0.0% (0/20)\n",
      "2017-01-06 20:42:49,114 : INFO : gram6-nationality-adjective: 0.0% (0/20)\n",
      "2017-01-06 20:42:49,126 : INFO : gram7-past-tense: 0.0% (0/20)\n",
      "2017-01-06 20:42:49,143 : INFO : gram8-plural: 0.0% (0/12)\n",
      "2017-01-06 20:42:49,151 : INFO : total: 0.0% (0/98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],\n",
       "  'section': 'family'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER')],\n",
       "  'section': 'gram3-comparative'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],\n",
       "  'section': 'gram4-superlative'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING')],\n",
       "  'section': 'gram5-present-participle'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],\n",
       "  'section': 'gram6-nationality-adjective'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID')],\n",
       "  'section': 'gram7-past-tense'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
       "  'section': 'gram8-plural'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HIS', 'HER', 'HE', 'SHE'),\n",
       "   ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "   ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
       "   ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "   ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
       "  'section': 'total'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(test_data_dir +'questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-06 19:43:59,264 : INFO : Pearson correlation coefficient against C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.1265\n",
      "2017-01-06 19:43:59,265 : INFO : Spearman rank-order correlation coefficient against C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.1470\n",
      "2017-01-06 19:43:59,266 : INFO : Pairs with unknown words ratio: 85.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.12648249887163787, 0.376463016725192),\n",
       " SpearmanrResult(correlation=0.14698495338762813, pvalue=0.3033560185328007),\n",
       " 85.55240793201133)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_word_pairs(test_data_dir +'wordsim353.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-06 20:08:56,721 : INFO : saving Word2Vec object under C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp, separately None\n",
      "2017-01-06 20:08:56,721 : INFO : not storing attribute cum_table\n",
      "2017-01-06 20:08:56,721 : INFO : not storing attribute syn0norm\n",
      "2017-01-06 20:08:56,775 : INFO : saved C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkstemp\n",
    "fs, temp_path = mkstemp(\"gensim_temp\")  # creates a temp file\n",
    "model.save(temp_path)  # save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tempfile.mkstemp([suffix=''[, prefix='tmp'[, dir=None[, text=False]]]])\n",
    "\n",
    "mkstemp方法用于创建一个临时文件。该方法仅仅用于创建临时文件，调用tempfile.mkstemp函数后，返回包含两个元素的元组，第一个元素指示操作该临时文件的安全级别，第二个元素指示该临时文件的路径。参数suffix和prefix分别表示临时文件名称的后缀和前缀；dir指定了临时文件所在的目录，如果没有指定目录，将根据系统环境变量TMPDIR, TEMP或者TMP的设置来保存临时文件；参数text指定了是否以文本的形式来操作文件，默认为False，表示以二进制的形式来操作文件。\n",
    "    \n",
    "http://www.cnblogs.com/captain_jack/archive/2011/01/19/1939555.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-06 20:09:08,989 : INFO : loading Word2Vec object from C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp\n",
      "2017-01-06 20:09:09,038 : INFO : loading wv recursively from C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp.wv.* with mmap=None\n",
      "2017-01-06 20:09:09,038 : INFO : setting ignored attribute cum_table to None\n",
      "2017-01-06 20:09:09,039 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-01-06 20:09:09,040 : INFO : loaded C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp\n"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec.load(temp_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online training / Resuming training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-06 20:15:22,227 : INFO : loading Word2Vec object from C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp\n",
      "2017-01-06 20:15:22,273 : INFO : loading wv recursively from C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp.wv.* with mmap=None\n",
      "2017-01-06 20:15:22,274 : INFO : setting ignored attribute cum_table to None\n",
      "2017-01-06 20:15:22,274 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-01-06 20:15:22,274 : INFO : loaded C:\\Users\\Student\\AppData\\Local\\Temp\\tmpe5ivks3bgensim_temp\n",
      "2017-01-06 20:15:22,278 : INFO : collecting all words and their counts\n",
      "2017-01-06 20:15:22,279 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-06 20:15:22,280 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences\n",
      "2017-01-06 20:15:22,281 : INFO : Updating model with new vocabulary\n",
      "2017-01-06 20:15:22,282 : INFO : New added 0 unique words (0% of original 13)\n",
      "                        and increased the count of 0 pre-existing words (0% of original 13)\n",
      "2017-01-06 20:15:22,282 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2017-01-06 20:15:22,283 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2017-01-06 20:15:22,283 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "2017-01-06 20:15:22,284 : INFO : estimated required memory for 1723 words and 200 dimensions: 3618300 bytes\n",
      "2017-01-06 20:15:22,288 : INFO : updating layer weights\n",
      "2017-01-06 20:15:22,290 : INFO : training model with 3 workers on 1723 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-01-06 20:15:22,291 : INFO : expecting 1 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-06 20:15:22,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-06 20:15:22,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-06 20:15:22,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-06 20:15:22,294 : INFO : training on 65 raw words (28 effective words) took 0.0s, 18643 effective words/s\n",
      "2017-01-06 20:15:22,295 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(temp_path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', \n",
    "                  'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, )\n",
    "\n",
    "# cleaning up temp\n",
    "os.close(fs)\n",
    "os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('area', 0.9950318932533264),\n",
       " ('bombings', 0.9949900507926941),\n",
       " (\"that's\", 0.994957447052002),\n",
       " ('pacific', 0.9949444532394409),\n",
       " ('boxing', 0.9949276447296143),\n",
       " ('12', 0.9949259757995605),\n",
       " ('bush', 0.9949249625205994),\n",
       " ('me', 0.9949047565460205),\n",
       " ('action', 0.9949044585227966),\n",
       " ('militant', 0.9949022531509399)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['human', 'crime'], negative=['party'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar(positive=[], negative=[], topn=10, restrict_vocab=None, indexer=None)\n",
    "\n",
    "Find the top-N most similar words. Positive words contribute positively towards the similarity, negative words negatively.\n",
    "\n",
    "This method computes cosine similarity between a simple mean of the projection weight vectors of the given words and the vectors for each word in the model. The method corresponds to the word-analogy and distance scripts in the original word2vec implementation.\n",
    "\n",
    "If topn is False, most_similar returns the vector of similarity scores.\n",
    "\n",
    "restrict_vocab is an optional integer which limits the range of vectors which are searched for most-similar values. For example, restrict_vocab=10000 would only check the first 10000 word vectors in the vocabulary order. (This may be meaningful if you’ve sorted the vocabulary by descending frequency.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tennis'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"input is lunch he sentence cat\".split())\n",
    "\n",
    "model.doesnt_match(\"cut appear have tennis\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doesnt_match(words)\n",
    "Which word from the given list doesn’t go with the others?\n",
    "\n",
    "Example:\n",
    "trained_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "'cereal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999567357806\n",
      "0.997924129753\n",
      "0.999888551396\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('human', 'party'))\n",
    "print(model.similarity('tree', 'murder'))\n",
    "print(model.similarity('royal', 'commission'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here don't look good because the training corpus is very small. To get meaningful results one needs to train on 500k+ words.\n",
    "\n",
    "If you need the raw output vectors in your application, you can access these either on a word-by-word basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01127922, -0.03585431, -0.0125974 , -0.04147318,  0.00118686,\n",
       "       -0.01606489, -0.00240359, -0.01758233, -0.03929804, -0.00437588,\n",
       "       -0.02706588, -0.04878673,  0.00651546, -0.00814761, -0.01902641,\n",
       "       -0.00802842, -0.02677576, -0.00783781, -0.00248508, -0.03722271,\n",
       "       -0.0401651 ,  0.02337405,  0.026868  , -0.00215164, -0.02530951,\n",
       "       -0.04220852, -0.00314844, -0.02795812,  0.04118952,  0.02010571,\n",
       "       -0.06484023, -0.03345084, -0.06004576,  0.02802508,  0.03755744,\n",
       "       -0.00150028,  0.02468759,  0.05103324,  0.00318254, -0.01412363,\n",
       "       -0.02588135, -0.04607787, -0.00969513, -0.01094116, -0.02884848,\n",
       "        0.0235644 , -0.01119853, -0.02832495,  0.01450322,  0.05574993,\n",
       "        0.02936582, -0.00019662, -0.00673731, -0.01590701, -0.01878649,\n",
       "       -0.02997692, -0.02445791, -0.04906164, -0.00614071, -0.0465661 ,\n",
       "       -0.009026  , -0.01483737, -0.00262535, -0.02153099, -0.04723297,\n",
       "        0.00348318, -0.00915821,  0.0257897 ,  0.05443203, -0.02771545,\n",
       "       -0.03547351, -0.06358738,  0.02810206, -0.00661734,  0.00591523,\n",
       "       -0.00530214, -0.01598136, -0.0154218 , -0.04425085,  0.01494798,\n",
       "       -0.00306561,  0.01014004,  0.02252189,  0.01370851, -0.00099842,\n",
       "        0.04073885, -0.01974431,  0.00881837, -0.00193436,  0.01870783,\n",
       "        0.01108071, -0.05681082, -0.0151587 ,  0.00155328,  0.01797221,\n",
       "       -0.01372595,  0.01848081, -0.02980921, -0.05438592,  0.02600546,\n",
       "        0.00580071, -0.01788255,  0.00940567,  0.05093832, -0.02726026,\n",
       "       -0.06574493, -0.00631127,  0.03604782, -0.03431922,  0.01238912,\n",
       "        0.04211594, -0.00404805,  0.06674642,  0.00657286, -0.04123365,\n",
       "        0.03209941, -0.01940445,  0.00641744, -0.0007858 ,  0.0257361 ,\n",
       "        0.03929282,  0.00282991,  0.07607613, -0.03510129,  0.01900587,\n",
       "       -0.0337614 , -0.00886983, -0.01680479,  0.00222792,  0.03838523,\n",
       "        0.03890262, -0.00041979, -0.04553364, -0.0002935 ,  0.00559755,\n",
       "       -0.00514073, -0.04137651,  0.03785593,  0.01920448,  0.02086805,\n",
       "        0.01726017,  0.0081133 , -0.02127644,  0.00757866,  0.00897926,\n",
       "       -0.00525119,  0.00404478,  0.01845771,  0.02991396, -0.00229874,\n",
       "        0.01563057, -0.00406435,  0.04994174, -0.0034782 , -0.0453846 ,\n",
       "       -0.00393162,  0.01385901, -0.00165312, -0.01670868,  0.04333138,\n",
       "       -0.00757191,  0.02972775, -0.03031535, -0.02085538, -0.03762035,\n",
       "       -0.01666641,  0.00238719, -0.04514377,  0.0065695 ,  0.0430431 ,\n",
       "        0.01727239, -0.01484694, -0.03610225,  0.03788262,  0.01109232,\n",
       "        0.03376326, -0.04752094,  0.0051838 , -0.00239808,  0.0310125 ,\n",
       "       -0.02195173, -0.05181452,  0.03119976,  0.05690091, -0.01019698,\n",
       "       -0.00457551,  0.00685219, -0.02878095, -0.00897455,  0.00861425,\n",
       "       -0.02591216,  0.00669582, -0.0150489 ,  0.03347906,  0.02492078,\n",
       "        0.00117683, -0.01329077,  0.02072382, -0.00704182,  0.03577595], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['tree']  # raw NumPy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "…or en-masse as a 2D NumPy matrix from model.syn0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "commit and push:\n",
    "https://github.com/isetbio/jupyter-hub-oauth-isetbio/wiki/Pushing-and-Pulling-Notebooks"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
